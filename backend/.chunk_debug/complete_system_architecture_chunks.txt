CHUNK ANALYSIS FOR: complete_system_architecture.md
====================================================================================================
Total Chunks: 30


====================================================================================================
CHUNK #1
====================================================================================================
Chunk ID: ab43a06e7f73a65ba1bf73151531419fce7c2a24
Kind: section
Lines: 1-2
Has Code: False
Header Level: 1
Section Path: AI Chat Application - Complete System Architecture & Flow

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow

# AI Chat Application - Complete System Architecture & Flow


====================================================================================================
CHUNK #2
====================================================================================================
Chunk ID: 16418bf5dd5d27a891fd55fb63924c6ffa8c212c
Kind: section
Lines: 3-16
Has Code: False
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Executive Summary

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Executive Summary

## Executive Summary

This is an AI-powered chat application with advanced RAG (Retrieval-Augmented Generation) capabilities. It supports multiple LLM providers (Ollama, LlamaCPP, HuggingFace, OpenAI) and can use either local or OpenAI-based embeddings for document retrieval.

**Key Features:**
- Real-time streaming chat interface
- Document upload and RAG-based context retrieval
- **Fusion Retrieval (RRF)** - Combines vector, BM25, and multi-query strategies
- Conversation history and session management
- Detailed statistics (tokens, sources, processing time)
- Support for multiple LLM and embedding providers

---


====================================================================================================
CHUNK #3
====================================================================================================
Chunk ID: c75856ab9c4611c4c9528fc1616f1b68c538116a
Kind: steps
Lines: 17-28
Has Code: False
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Table of Contents

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Table of Contents

## Table of Contents
1. [System Architecture](#system-architecture)
2. [Application Startup Flow](#application-startup-flow)
3. [Chat Flow - Without RAG](#chat-flow---without-rag)
4. [Chat Flow - With Simple RAG](#chat-flow---with-simple-rag)
5. [Chat Flow - With Fusion Retrieval (RRF)](#chat-flow---with-fusion-retrieval-rrf)
6. [Document Upload Flow](#document-upload-flow)
7. [Component Details](#component-details)

---


====================================================================================================
CHUNK #4
====================================================================================================
Chunk ID: 33819e83c25fb3d63eacb5a2e626d6646f7fd31a
Kind: section
Lines: 29-63
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > System Architecture
Commands: ┌─────────────────────────────────────────────────────────────┐, │                      Frontend (Next.js)                      │, │  ┌────────────┐  ┌──────────────┐  ┌──────────────────┐    │, │  │   Chat UI  │  │  useChat Hook │  │  SSE Event       │    │, │  │            │──│               │──│  Handler         │    │, │  └────────────┘  └──────────────┘  └──────────────────┘    │, └────────────────────────────┬────────────────────────────────┘,                              │ HTTP/SSE,                              ▼, ┌─────────────────────────────────────────────────────────────┐, │                    Backend (FastAPI)                         │, │  ┌──────────────┐  ┌──────────────┐  ┌──────────────────┐  │, │  │   Routers    │  │   Services   │  │   Providers      │  │, │  │  /api/chat   │──│  - Graph     │──│  - Ollama        │  │, │  │  /api/docs   │  │  - RAG       │  │  - LlamaCPP      │  │, │  │              │  │  - Fusion    │  │  - HuggingFace   │  │, │  │              │  │  - Session   │  │  - OpenAI        │  │, │  └──────────────┘  └──────────────┘  └──────────────────┘  │, └────────────────────────────┬────────────────────────────────┘,                              │,                              ▼, ┌─────────────────────────────────────────────────────────────┐, │                    Data Layer (ChromaDB)                     │, │  ┌──────────────┐  ┌──────────────┐  ┌──────────────────┐  │, │  │  Documents   │  │  Sessions    │  │  Embeddings      │  │, │  │  Collection  │  │  Collection  │  │  (Local/OpenAI)  │  │, │  └──────────────┘  └──────────────┘  └──────────────────┘  │, └─────────────────────────────────────────────────────────────┘

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > System Architecture

## System Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                      Frontend (Next.js)                      │
│  ┌────────────┐  ┌──────────────┐  ┌──────────────────┐    │
│  │   Chat UI  │  │  useChat Hook │  │  SSE Event       │    │
│  │            │──│               │──│  Handler         │    │
│  └────────────┘  └──────────────┘  └──────────────────┘    │
└────────────────────────────┬────────────────────────────────┘
                             │ HTTP/SSE
                             ▼
┌─────────────────────────────────────────────────────────────┐
│                    Backend (FastAPI)                         │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────────┐  │
│  │   Routers    │  │   Services   │  │   Providers      │  │
│  │  /api/chat   │──│  - Graph     │──│  - Ollama        │  │
│  │  /api/docs   │  │  - RAG       │  │  - LlamaCPP      │  │
│  │              │  │  - Fusion    │  │  - HuggingFace   │  │
│  │              │  │  - Session   │  │  - OpenAI        │  │
│  └──────────────┘  └──────────────┘  └──────────────────┘  │
└────────────────────────────┬────────────────────────────────┘
                             │
                             ▼
┌─────────────────────────────────────────────────────────────┐
│                    Data Layer (ChromaDB)                     │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────────┐  │
│  │  Documents   │  │  Sessions    │  │  Embeddings      │  │
│  │  Collection  │  │  Collection  │  │  (Local/OpenAI)  │  │
│  └──────────────┘  └──────────────┘  └──────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

---


====================================================================================================
CHUNK #5
====================================================================================================
Chunk ID: f6f75820a2412cabde0cdbaaf5c4750a30de355d_merged
Kind: merged_section
Lines: 64-163
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Application Startup Flow

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Application Startup Flow

## Application Startup Flow

Section: AI Chat Application - Complete System Architecture & Flow > Application Startup Flow > Backend Initialization

### Backend Initialization

```mermaid
graph TD
    A[Start Backend] --> B[Load Configuration]
    B --> C[Initialize Embedding Provider]
    C --> D{OpenAI Embeddings?}
    D -->|Yes| E[Initialize OpenAI Embeddings]
    D -->|No| F[Load SentenceTransformer Model]
    E --> G[Initialize ChromaDB]
    F --> G
    G --> H[Get/Create Documents Collection]
    H --> I[Auto-load Documents from /data]
    I --> J[Initialize Session Service]
    J --> K[Get/Create Sessions Collection]
    K --> L[Initialize LLM Provider]
    L --> M{Provider Type?}
    M -->|Ollama| N[Connect to Ollama API]
    M -->|LlamaCPP| O[Load GGUF Model]
    M -->|HuggingFace| P[Load HF Model]
    M -->|OpenAI| Q[Connect to OpenAI API]
    N --> R[Test Connection]
    O --> R
    P --> R
    Q --> R
    R --> S[Backend Ready]
```

**Detailed Steps:**
[CODE BLOCK - Languages: mermaid]

Section: AI Chat Application - Complete System Architecture & Flow > Application Startup Flow > Backend Initialization

### Backend Initialization
1. **Configuration Loading** (`config.py`)
   - Loads environment variables from `.env`
   - Sets provider type, model paths, API endpoints
   - Configures embedding settings

2. **Embedding Provider Initialization** (`embedding_service.py`)
   - **Local Mode**: Downloads and loads SentenceTransformer model (all-MiniLM-L6-v2)
   - **OpenAI Mode**: Initializes OpenAI client with API key or SSO token
   - Logs: `"Using local embeddings"` or `"Using OpenAI embeddings"`

3. **ChromaDB Initialization** (`rag_service.py`)
   - Creates persistent client at `backend/data/chroma`
   - Gets or creates `documents` collection with cosine similarity
   - Logs: `"ChromaDB initialized. Collection 'documents' has X documents"`

4. **Document Auto-Loading** (`document_loader.py`)
   - Scans `backend/data/` for PDF, TXT, MD files
   - Checks `.loaded_docs` registry to skip already-loaded files
   - For new files:
     - Extracts text (with OCR for scanned PDFs)
     - Chunks text (1000 chars with 200 char overlap)
     - Generates embeddings for each chunk
     - Stores in ChromaDB with metadata
   - Logs: `"Document loading complete. New docs: X, Total in RAG: Y"`

5. **Session Service Initialization** (`session_service.py`)
   - Gets or creates `chat_sessions` collection
   - Initializes in-memory cache for conversation history
   - Logs: `"Session Service initialized. Sessions collection has X entries"`

6. **LLM Provider Initialization** (`llm_service.py`, `providers/`)
   - Creates provider based on `PROVIDER_TYPE` config
   - **Ollama**: Connects to local Ollama server, tests with simple prompt
   - **LlamaCPP**: Loads GGUF model file into memory
   - **HuggingFace**: Downloads and loads model from HF Hub
   - **OpenAI**: Initializes API client with credentials
   - Logs: `"Provider X initialized successfully!"` or error if failed

Section: AI Chat Application - Complete System Architecture & Flow > Application Startup Flow > Frontend Initialization

### Frontend Initialization

```mermaid
graph TD
    A[Start Frontend] --> B[Load Next.js App]
    B --> C[Initialize useChat Hook]
    C --> D[Check localStorage for session_id]
    D -->|Found| E[Load Existing Session]
    D -->|Not Found| F[Generate New UUID]
    F --> G[Save to localStorage]
    E --> H[Render Chat UI]
    G --> H
    H --> I[Frontend Ready]
```

**Detailed Steps:**
[CODE BLOCK - Languages: mermaid]

Section: AI Chat Application - Complete System Architecture & Flow > Application Startup Flow > Frontend Initialization

### Frontend Initialization
1. **Session Management** (`useChat.ts`)
   - Checks `localStorage` for `ai_chat_session_id`
   - If exists: Uses existing session (preserves conversation history)
   - If not: Generates new UUID and saves to localStorage
   - Session ID is sent with every chat request

2. **UI Rendering** (`page.tsx`)
   - Renders chat interface with message list, input, and controls
   - Initializes state for messages, loading, errors
   - Sets up file upload handler

---


====================================================================================================
CHUNK #6
====================================================================================================
Chunk ID: 5a18000f74226a71bb08fe61380dc9a6a002e160
Kind: narrative
Lines: 164-247
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Chat Flow - Without RAG
Commands: sequenceDiagram,     participant U as User,     participant F as Frontend,     participant B as Backend API,     participant G as Graph Service,     participant S as Session Service,     participant L as LLM Provider,     U->>F: Type message & send,     F->>F: Add user message to UI,     F->>F: Create placeholder for assistant,     F->>B: POST /api/chat/stream,     Note over F,B: {message, conversation_id, use_rag: false},     B->>G: chat_stream(),     G->>S: get_conversation_history(),     S-->>G: Previous messages (last 6),     G->>G: Create system prompt with history,     Note over G: "You are a helpful AI assistant..."<br/>+ conversation history,     G->>L: generate_stream(prompt),     loop For each token,         L-->>G: Token,         G-->>B: {token: "..."},         B-->>F: SSE: event=token,         F->>F: Append token to message,     end,     G->>G: Calculate statistics,     G-->>B: {statistics: {...}},     B-->>F: SSE: event=statistics,     F->>F: Attach statistics to message,     G->>S: save_message(user),     G->>S: save_message(assistant),     B-->>F: SSE: event=end,     F->>F: Mark loading complete

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Chat Flow - Without RAG

## Chat Flow - Without RAG

**User Action**: User types message and clicks send (RAG toggle OFF)

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant B as Backend API
    participant G as Graph Service
    participant S as Session Service
    participant L as LLM Provider
    
    U->>F: Type message & send
    F->>F: Add user message to UI
    F->>F: Create placeholder for assistant
    F->>B: POST /api/chat/stream
    Note over F,B: {message, conversation_id, use_rag: false}
    
    B->>G: chat_stream()
    G->>S: get_conversation_history()
    S-->>G: Previous messages (last 6)
    
    G->>G: Create system prompt with history
    Note over G: "You are a helpful AI assistant..."<br/>+ conversation history
    
    G->>L: generate_stream(prompt)
    
    loop For each token
        L-->>G: Token
        G-->>B: {token: "..."}
        B-->>F: SSE: event=token
        F->>F: Append token to message
    end
    
    G->>G: Calculate statistics
    G-->>B: {statistics: {...}}
    B-->>F: SSE: event=statistics
    F->>F: Attach statistics to message
    
    G->>S: save_message(user)
    G->>S: save_message(assistant)
    
    B-->>F: SSE: event=end
    F->>F: Mark loading complete
```

**Step-by-Step:**
[CODE BLOCK - Languages: mermaid]


====================================================================================================
CHUNK #7
====================================================================================================
Chunk ID: 13b18fa9254525db9e943a47dd91840883d88569
Kind: steps
Lines: 164-247
Has Code: False
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Chat Flow - Without RAG

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Chat Flow - Without RAG

## Chat Flow - Without RAG
1. **Frontend** (`useChat.ts:sendMessage`)
   - Adds user message to state immediately
   - Creates empty assistant message placeholder
   - Initiates SSE connection to `/api/chat/stream`

2. **Backend Router** (`chat.py:chat_stream`)
   - Receives request with `use_rag: false`
   - Calls `stream_generator()` which calls `graph_service.chat_stream()`

3. **Graph Service** (`graph_service.py:chat_stream`)
   - Retrieves last 6 messages from conversation history
   - Creates system prompt with conversation context
   - Calls LLM provider's `generate_stream()`

4. **Token Streaming**
   - LLM generates tokens one by one
   - Each token is yielded as `{token: "..."}`
   - Router emits SSE event: `event: token`
   - Frontend appends each token to assistant message in real-time

5. **Statistics Calculation**
   - After streaming completes:
     - Calculates input/output tokens (word count × 1.3)
     - Measures processing time
     - Yields `{statistics: {...}}`
   - Router emits SSE event: `event: statistics`
   - Frontend attaches statistics to message

6. **Session Storage**
   - Saves user message to ChromaDB sessions collection
   - Saves assistant response to ChromaDB sessions collection
   - Messages available for future conversation context

---


====================================================================================================
CHUNK #8
====================================================================================================
Chunk ID: 0d736d7316672fd7c3d11a69bb3b848b08495820
Kind: narrative
Lines: 248-350
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Chat Flow - With Simple RAG
Commands: sequenceDiagram,     participant U as User,     participant F as Frontend,     participant B as Backend API,     participant G as Graph Service,     participant R as RAG Service,     participant E as Embedding Provider,     participant C as ChromaDB,     participant L as LLM Provider,     U->>F: Type message & send (RAG ON),     F->>B: POST /api/chat/stream,     Note over F,B: {message, use_rag: true},     B->>G: chat_stream(use_rag=true),     G->>E: embed_query(message),     E-->>G: Query embedding vector,     G->>R: search(query, n_results=3),     R->>C: query(embedding, n=3),     C-->>R: Top 3 similar documents,     R-->>G: Documents with metadata,     G->>G: Build context from documents,     Note over G: [Document 1]<br/>content...<br/>[Document 2]<br/>content...,     G->>G: Create enhanced prompt,     Note over G: System prompt<br/>+ conversation history<br/>+ RAG context<br/>+ user message,     G->>L: generate_stream(enhanced_prompt),     loop For each token,         L-->>G: Token,         G-->>B: {token: "..."},         B-->>F: SSE: event=token,     end,     G->>G: Calculate statistics,     Note over G: Include source documents,     G-->>B: {statistics: {source_documents: [...]}},     B-->>F: SSE: event=statistics,     F->>F: Display sources in UI

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Chat Flow - With Simple RAG

## Chat Flow - With Simple RAG

**User Action**: User types message and clicks send (RAG toggle ON)

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant B as Backend API
    participant G as Graph Service
    participant R as RAG Service
    participant E as Embedding Provider
    participant C as ChromaDB
    participant L as LLM Provider
    
    U->>F: Type message & send (RAG ON)
    F->>B: POST /api/chat/stream
    Note over F,B: {message, use_rag: true}
    
    B->>G: chat_stream(use_rag=true)
    
    G->>E: embed_query(message)
    E-->>G: Query embedding vector
    
    G->>R: search(query, n_results=3)
    R->>C: query(embedding, n=3)
    C-->>R: Top 3 similar documents
    R-->>G: Documents with metadata
    
    G->>G: Build context from documents
    Note over G: [Document 1]<br/>content...<br/>[Document 2]<br/>content...
    
    G->>G: Create enhanced prompt
    Note over G: System prompt<br/>+ conversation history<br/>+ RAG context<br/>+ user message
    
    G->>L: generate_stream(enhanced_prompt)
    
    loop For each token
        L-->>G: Token
        G-->>B: {token: "..."}
        B-->>F: SSE: event=token
    end
    
    G->>G: Calculate statistics
    Note over G: Include source documents
    G-->>B: {statistics: {source_documents: [...]}}
    B-->>F: SSE: event=statistics
    F->>F: Display sources in UI
```

**Step-by-Step:**
[CODE BLOCK - Languages: mermaid]


====================================================================================================
CHUNK #9
====================================================================================================
Chunk ID: 2a9802e27185495c44c7031b8dfcb2bf61ccfa34
Kind: steps
Lines: 248-350
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Chat Flow - With Simple RAG
Commands:      [Document 1],      <content from chunk 1>,      [Document 2],      <content from chunk 2>,      You are a helpful AI assistant...,      Previous conversation:,      User: What is X?,      Assistant: X is...,      Use the following context:,      [Document 1],      X is a software platform...,      User: Tell me more about its features,      Assistant:

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Chat Flow - With Simple RAG

## Chat Flow - With Simple RAG
1. **Query Embedding** (`graph_service.py`)
   - User's message is embedded using the embedding provider
   - **Local**: SentenceTransformer generates embedding locally
   - **OpenAI**: API call to generate embedding

2. **Document Retrieval** (`rag_service.py:search`)
   - Queries ChromaDB with embedding vector
   - Uses cosine similarity to find top 3 most relevant chunks
   - Returns documents with metadata (source, chunk_index, distance)

3. **Context Building**
   - Formats retrieved documents as numbered context:
     ```
     [Document 1]
     <content from chunk 1>
     
     [Document 2]
     <content from chunk 2>
     ```

4. **Enhanced Prompt Creation**
   - System prompt + conversation history + RAG context + user message
   - Example:
     ```
     You are a helpful AI assistant...
     
     Previous conversation:
     User: What is X?
     Assistant: X is...
     
     Use the following context:
     [Document 1]
     X is a software platform...
     
     User: Tell me more about its features
     Assistant:
     ```

5. **Response Generation**
   - LLM generates response using enhanced context
   - Streams tokens in real-time

6. **Statistics with Sources**
   - Statistics include source document metadata:
     - Filename
     - Chunk position (e.g., "Chunk 2 of 12")
     - Relevance score
   - Frontend displays collapsible source list

---


====================================================================================================
CHUNK #10
====================================================================================================
Chunk ID: 5ea46e7659ac1d42321131b638b18b5be177391f
Kind: narrative
Lines: 351-480
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Chat Flow - With Fusion Retrieval (RRF)
Commands: sequenceDiagram,     participant U as User,     participant F as Frontend,     participant B as Backend API,     participant G as Graph Service,     participant FR as Fusion RAG Service,     participant E as Embedding Provider,     participant BM as BM25 Index,     participant L as LLM Provider,     participant C as ChromaDB,     U->>F: Type question (RAG + Fusion ON),     F->>B: POST /api/chat/stream,     Note over F,B: {message, use_rag: true,<br/>use_fusion: true},     B->>G: chat_stream(use_fusion=true),     G->>FR: fusion_search_with_diversity(),     Note over FR: Multi-Strategy Retrieval,     par Strategy 1: Vector Search,         FR->>E: embed_query(message),         E-->>FR: Query embedding,         FR->>C: query(embedding, n=10),         C-->>FR: Top 10 by vector similarity,     and Strategy 2: BM25 Keyword,         FR->>BM: Build/use BM25 index,         BM-->>FR: Top 10 by keyword match,     and Strategy 3: Multi-Query,         FR->>L: Generate query variations,         L-->>FR: 2-3 query variations,         FR->>C: Search each variation,         C-->>FR: Results per variation,     end,     FR->>FR: Apply Reciprocal Rank Fusion,     Note over FR: Combine rankings:<br/>RRF score = Σ(1/(k+rank))<br/>k=60 (default),     FR->>FR: Enforce source diversity,     Note over FR: Ensure min 2 unique sources<br/>Deduplicate by content,     FR-->>G: Top 5 documents with RRF scores,     G->>G: Build enhanced context,     Note over G: [Document 1 - Source: X - RRF: 0.85]<br/>content...,     G->>L: generate_stream(enhanced_prompt),     loop For each token,         L-->>G: Token,         G-->>B: {token: "..."},         B-->>F: SSE: event=token,     end,     G-->>B: {statistics: {<br/>num_documents: 5,<br/>unique_sources: 3,<br/>strategies: ["vector", "bm25", "multi_query"]}},     B-->>F: SSE: event=statistics,     F->>F: Display fusion statistics

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Chat Flow - With Fusion Retrieval (RRF)

## Chat Flow - With Fusion Retrieval (RRF)

**User Action**: User types question with Fusion Retrieval enabled (RAG + Fusion ON)

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant B as Backend API
    participant G as Graph Service
    participant FR as Fusion RAG Service
    participant E as Embedding Provider
    participant BM as BM25 Index
    participant L as LLM Provider
    participant C as ChromaDB
    
    U->>F: Type question (RAG + Fusion ON)
    F->>B: POST /api/chat/stream
    Note over F,B: {message, use_rag: true,<br/>use_fusion: true}
    
    B->>G: chat_stream(use_fusion=true)
    G->>FR: fusion_search_with_diversity()
    
    Note over FR: Multi-Strategy Retrieval
    
    par Strategy 1: Vector Search
        FR->>E: embed_query(message)
        E-->>FR: Query embedding
        FR->>C: query(embedding, n=10)
        C-->>FR: Top 10 by vector similarity
    and Strategy 2: BM25 Keyword
        FR->>BM: Build/use BM25 index
        BM-->>FR: Top 10 by keyword match
    and Strategy 3: Multi-Query
        FR->>L: Generate query variations
        L-->>FR: 2-3 query variations
        FR->>C: Search each variation
        C-->>FR: Results per variation
    end
    
    FR->>FR: Apply Reciprocal Rank Fusion
    Note over FR: Combine rankings:<br/>RRF score = Σ(1/(k+rank))<br/>k=60 (default)
    
    FR->>FR: Enforce source diversity
    Note over FR: Ensure min 2 unique sources<br/>Deduplicate by content
    
    FR-->>G: Top 5 documents with RRF scores
    
    G->>G: Build enhanced context
    Note over G: [Document 1 - Source: X - RRF: 0.85]<br/>content...
    
    G->>L: generate_stream(enhanced_prompt)
    
    loop For each token
        L-->>G: Token
        G-->>B: {token: "..."}
        B-->>F: SSE: event=token
    end
    
    G-->>B: {statistics: {<br/>num_documents: 5,<br/>unique_sources: 3,<br/>strategies: ["vector", "bm25", "multi_query"]}}
    B-->>F: SSE: event=statistics
    F->>F: Display fusion statistics
```

**Step-by-Step:**
[CODE BLOCK - Languages: mermaid]


====================================================================================================
CHUNK #11
====================================================================================================
Chunk ID: 5dbea94fcbe91b10e674d59f71f0cc0e84dd0914
Kind: steps
Lines: 351-480
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Chat Flow - With Fusion Retrieval (RRF)
Commands:      Document A: Vector rank=1, BM25 rank=3, Multi-query rank=2,      RRF = 1/(60+1) + 1/(60+3) + 1/(60+2) = 0.0164 + 0.0159 + 0.0161 = 0.0484,      [Document 1 - Source: guide.pdf - RRF Score: 0.856],      Content from the document...,      [Document 2 - Source: manual.txt - RRF Score: 0.742],      Content from another document...,      [Fusion Search Stats: 5 docs from 3 sources using vector, bm25, multi_query]

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Chat Flow - With Fusion Retrieval (RRF)

## Chat Flow - With Fusion Retrieval (RRF)
1. **Fusion Retrieval Initialization** (`fusion_rag_service.py`)
   - Checks if BM25 index is built, builds if needed
   - Prepares to run multiple retrieval strategies in parallel

2. **Multi-Strategy Retrieval**
   
   **Strategy 1 - Vector Semantic Search:**
   - Embeds the user query
   - Queries ChromaDB for top 10 semantically similar documents
   - Uses cosine similarity on embeddings
   
   **Strategy 2 - BM25 Keyword Search:**
   - Uses BM25Okapi algorithm for keyword matching
   - Scores documents based on term frequency and document length
   - Returns top 10 documents by BM25 score
   
   **Strategy 3 - Multi-Query Expansion:**
   - Uses LLM to generate 2-3 variations of the original query
   - Examples: "What is X?" → ["Explain X", "Definition of X", "X overview"]
   - Searches each variation independently
   - Combines and deduplicates results

3. **Reciprocal Rank Fusion (RRF)**
   - Combines rankings from all strategies using RRF algorithm
   - Formula: `RRF_score = Σ(1 / (k + rank))` where k=60
   - Documents appearing in multiple strategies get higher scores
   - Normalizes scores to 0-1 range
   - Example:
     ```
     Document A: Vector rank=1, BM25 rank=3, Multi-query rank=2
     RRF = 1/(60+1) + 1/(60+3) + 1/(60+2) = 0.0164 + 0.0159 + 0.0161 = 0.0484
     ```

4. **Source Diversity Enforcement**
   - Sorts documents by RRF score
   - Ensures results come from at least `min_sources` (default: 2) unique files
   - Removes near-duplicate content using similarity comparison
   - Returns top N documents with maximum diversity

5. **Context Building**
   - Formats documents with metadata:
     ```
     [Document 1 - Source: guide.pdf - RRF Score: 0.856]
     Content from the document...
     
     [Document 2 - Source: manual.txt - RRF Score: 0.742]
     Content from another document...
     
     [Fusion Search Stats: 5 docs from 3 sources using vector, bm25, multi_query]
     ```

6. **Enhanced Response Generation**
   - LLM receives rich context from multiple retrieval strategies
   - Better coverage across different document sources
   - Higher quality answers for complex, multi-faceted queries

7. **Statistics with Fusion Details**
   - Shows number of documents retrieved
   - Lists unique source files
   - Displays which strategies were used
   - Shows RRF scores for transparency

---


====================================================================================================
CHUNK #12
====================================================================================================
Chunk ID: be288ea0d2bc2a01997ae90922dc278fd155bcfc
Kind: narrative
Lines: 481-579
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Document Upload Flow
Commands: sequenceDiagram,     participant U as User,     participant F as Frontend,     participant B as Backend API,     participant DS as Document Service,     participant DL as Document Loader,     participant E as Embedding Provider,     participant R as RAG Service,     participant C as ChromaDB,     U->>F: Select file & upload,     F->>B: POST /api/documents/upload,     Note over F,B: FormData with file,     B->>DS: upload_document(file),     DS->>DS: Calculate MD5 hash,     DS->>DS: Check hash registry,     alt File is duplicate,         DS-->>B: {status: "duplicate"},         B-->>F: Duplicate message,     else New file,         DS->>DS: Save file to uploads/,         DS->>DL: extract_text(file),         alt PDF file,             DL->>DL: Try standard extraction,             alt Scanned PDF,                 DL->>DL: OCR with Tesseract,             end,             DL->>DL: Extract tables,             DL->>DL: Extract images & OCR,         else TXT/MD file,             DL->>DL: Read text directly,         end,         DL-->>DS: Extracted text,         DS->>DL: chunk_text(text),         DL-->>DS: Text chunks (1000 chars, 200 overlap),         loop For each chunk,             DS->>E: embed_documents([chunk]),             E-->>DS: Embedding vector,             DS->>R: add_document(chunk, metadata),             R->>C: Store chunk + embedding + metadata,         end,         DS->>DS: Update hash registry,         DS-->>B: {status: "uploaded", chunks: N},         B-->>F: Success message,         F->>F: Show success notification,     end

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Document Upload Flow

## Document Upload Flow

**User Action**: User uploads PDF/TXT/MD file

```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant B as Backend API
    participant DS as Document Service
    participant DL as Document Loader
    participant E as Embedding Provider
    participant R as RAG Service
    participant C as ChromaDB
    
    U->>F: Select file & upload
    F->>B: POST /api/documents/upload
    Note over F,B: FormData with file
    
    B->>DS: upload_document(file)
    DS->>DS: Calculate MD5 hash
    DS->>DS: Check hash registry
    
    alt File is duplicate
        DS-->>B: {status: "duplicate"}
        B-->>F: Duplicate message
    else New file
        DS->>DS: Save file to uploads/
        DS->>DL: extract_text(file)
        
        alt PDF file
            DL->>DL: Try standard extraction
            alt Scanned PDF
                DL->>DL: OCR with Tesseract
            end
            DL->>DL: Extract tables
            DL->>DL: Extract images & OCR
        else TXT/MD file
            DL->>DL: Read text directly
        end
        
        DL-->>DS: Extracted text
        DS->>DL: chunk_text(text)
        DL-->>DS: Text chunks (1000 chars, 200 overlap)
        
        loop For each chunk
            DS->>E: embed_documents([chunk])
            E-->>DS: Embedding vector
            DS->>R: add_document(chunk, metadata)
            R->>C: Store chunk + embedding + metadata
        end
        
        DS->>DS: Update hash registry
        DS-->>B: {status: "uploaded", chunks: N}
        B-->>F: Success message
        F->>F: Show success notification
    end
```

**Step-by-Step:**
[CODE BLOCK - Languages: mermaid]


====================================================================================================
CHUNK #13
====================================================================================================
Chunk ID: 73a88c7a859159fe1e55ba20910f7f2112d7823e
Kind: steps
Lines: 481-579
Has Code: False
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Document Upload Flow

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Document Upload Flow

## Document Upload Flow
1. **File Validation** (`document_service.py`)
   - Checks file extension (must be .pdf, .txt, or .md)
   - Calculates MD5 hash of file content

2. **Deduplication Check**
   - Looks up hash in `.document_hashes.json` registry
   - If found: Returns duplicate status, skips processing
   - If new: Proceeds with processing

3. **Text Extraction** (`document_loader.py`)
   - **PDF**: 
     - Tries standard text extraction with pypdf
     - If scanned (< 50 chars extracted): Uses OCR with Tesseract
     - Extracts tables with pdfplumber, converts to markdown
     - Extracts images and runs OCR on them
   - **TXT/MD**: Direct text read

4. **Text Chunking**
   - Splits text into 1000-character chunks
   - 200-character overlap between chunks for context continuity
   - Tries to break at sentence boundaries

5. **Embedding Generation**
   - **Local Mode**: SentenceTransformer encodes each chunk
   - **OpenAI Mode**: Batches chunks (max 32 per API call) for efficiency

6. **Storage in ChromaDB**
   - Each chunk stored with:
     - Content text
     - Embedding vector
     - Metadata: source filename, chunk_index, total_chunks, file_type, file_size, upload_timestamp, content_hash

7. **Registry Update**
   - Adds file hash to registry with chunk count and doc IDs
   - Prevents future duplicate uploads

---


====================================================================================================
CHUNK #14
====================================================================================================
Chunk ID: f9a37e3f9224b71823853d228d64db968be527a6_merged
Kind: merged_section
Lines: 580-583
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Component Details

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Component Details

## Component Details

Section: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services

### Backend Services


====================================================================================================
CHUNK #15
====================================================================================================
Chunk ID: fa9d28012c1820cab01591d72b54c635ab89d43e
Kind: section
Lines: 584-595
Has Code: False
Header Level: 4
Section Path: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services > Graph Service (`graph_service.py`)

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services > Graph Service (`graph_service.py`)

#### Graph Service (`graph_service.py`)
- **Purpose**: Orchestrates the chat workflow using LangGraph
- **Key Methods**:
  - `chat()`: Non-streaming chat
  - `chat_stream()`: Streaming chat with statistics
- **Responsibilities**:
  - Retrieve conversation history
  - Coordinate RAG retrieval
  - Build prompts with context
  - Stream LLM responses
  - Track and return statistics


====================================================================================================
CHUNK #16
====================================================================================================
Chunk ID: 6d94d92983280c5b35755770c6651354df77e686
Kind: section
Lines: 596-603
Has Code: False
Header Level: 4
Section Path: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services > RAG Service (`rag_service.py`)

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services > RAG Service (`rag_service.py`)

#### RAG Service (`rag_service.py`)
- **Purpose**: Manages document storage and retrieval
- **Key Methods**:
  - `add_document()`: Store document chunk with embedding
  - `search()`: Find similar documents
  - `get_context()`: Format documents as context string
- **Uses**: ChromaDB for vector storage


====================================================================================================
CHUNK #17
====================================================================================================
Chunk ID: 21872f0621cd1e107b2a17f78c49c1963a63736c
Kind: section
Lines: 604-610
Has Code: False
Header Level: 4
Section Path: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services > Session Service (`session_service.py`)

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services > Session Service (`session_service.py`)

#### Session Service (`session_service.py`)
- **Purpose**: Manages conversation history
- **Key Methods**:
  - `add_message()`: Store user/assistant message
  - `get_conversation_history()`: Retrieve recent messages
- **Features**: In-memory caching for performance


====================================================================================================
CHUNK #18
====================================================================================================
Chunk ID: e2048fd2b421ded80b30978cf977fa4f7a0b8148
Kind: section
Lines: 611-628
Has Code: False
Header Level: 4
Section Path: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services > Fusion RAG Service (`fusion_rag_service.py`)

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services > Fusion RAG Service (`fusion_rag_service.py`)

#### Fusion RAG Service (`fusion_rag_service.py`)
- **Purpose**: Implements advanced fusion retrieval using Reciprocal Rank Fusion (RRF)
- **Key Methods**:
  - `fusion_search_with_diversity()`: Multi-strategy search with source diversity
  - `reciprocal_rank_fusion()`: Combines multiple retrieval strategies
  - `bm25_search()`: Keyword-based search using BM25
  - `multi_query_retrieval()`: Query expansion and retrieval
  - `build_bm25_index()`: Builds BM25 index from ChromaDB documents
- **Strategies**: 
  - Vector semantic search (cosine similarity)
  - BM25 keyword search (term frequency)
  - Multi-query expansion (LLM-generated variations)
- **Features**:
  - Reciprocal Rank Fusion algorithm for score aggregation
  - Source diversity enforcement
  - Content deduplication
  - Configurable number of results and minimum sources


====================================================================================================
CHUNK #19
====================================================================================================
Chunk ID: 309b1a35fff3b22c084885448b6ab9f9a9f93420
Kind: section
Lines: 629-634
Has Code: False
Header Level: 4
Section Path: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services > Embedding Service (`embedding_service.py`)

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Component Details > Backend Services > Embedding Service (`embedding_service.py`)

#### Embedding Service (`embedding_service.py`)
- **Purpose**: Provides embedding generation abstraction
- **Providers**:
  - `LocalEmbeddingProvider`: SentenceTransformer
  - `OpenAIEmbeddingProvider`: OpenAI API with batching


====================================================================================================
CHUNK #20
====================================================================================================
Chunk ID: e5526193eb2dc04bc41c048e8ec8a57a946612d0
Kind: section
Lines: 635-636
Has Code: False
Header Level: 3
Section Path: AI Chat Application - Complete System Architecture & Flow > Component Details > Frontend Components

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Component Details > Frontend Components

### Frontend Components


====================================================================================================
CHUNK #21
====================================================================================================
Chunk ID: 5c071e2772c6e2e23567a008ae3ae44184cb5fa8
Kind: section
Lines: 637-644
Has Code: False
Header Level: 4
Section Path: AI Chat Application - Complete System Architecture & Flow > Component Details > Frontend Components > useChat Hook (`useChat.ts`)

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Component Details > Frontend Components > useChat Hook (`useChat.ts`)

#### useChat Hook (`useChat.ts`)
- **Purpose**: Manages chat state and API communication
- **Features**:
  - Session management
  - SSE event handling
  - Message state management
  - File upload handling


====================================================================================================
CHUNK #22
====================================================================================================
Chunk ID: 185f28aaff7efd87a800a80b45fe52a96527f68d
Kind: section
Lines: 645-651
Has Code: False
Header Level: 4
Section Path: AI Chat Application - Complete System Architecture & Flow > Component Details > Frontend Components > MessageBubble (`MessageBubble.tsx`)

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Component Details > Frontend Components > MessageBubble (`MessageBubble.tsx`)

#### MessageBubble (`MessageBubble.tsx`)
- **Purpose**: Renders individual messages
- **Features**:
  - Markdown rendering
  - Code syntax highlighting
  - Statistics display (collapsible)


====================================================================================================
CHUNK #23
====================================================================================================
Chunk ID: 4c64d885fd27a02d712e9c2865db8d5b7f540eef
Kind: section
Lines: 652-661
Has Code: False
Header Level: 4
Section Path: AI Chat Application - Complete System Architecture & Flow > Component Details > Frontend Components > MessageStatistics (`MessageStatistics.tsx`)

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Component Details > Frontend Components > MessageStatistics (`MessageStatistics.tsx`)

#### MessageStatistics (`MessageStatistics.tsx`)
- **Purpose**: Displays response statistics
- **Shows**:
  - Token counts (input/output/total)
  - Source documents with relevance scores
  - Processing time in milliseconds
  - Fusion retrieval metadata (strategies used, unique sources)

---


====================================================================================================
CHUNK #24
====================================================================================================
Chunk ID: b5129af58e0e692d9c33ed4491535ab8cc1689e9_merged
Kind: merged_section
Lines: 662-694
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Configuration

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Configuration

## Configuration

Section: AI Chat Application - Complete System Architecture & Flow > Configuration > Environment Variables

### Environment Variables

```bash
# Provider Selection
PROVIDER_TYPE=ollama  # ollama, llamacpp, huggingface, openai

# LLM Settings
TEMPERATURE=0.7
MAX_TOKENS=512

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=gpt-oss:latest

# OpenAI (for LLM)
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_API_KEY=sk-...
OPENAI_SSO_TOKEN=...  # Alternative to API key

# Embeddings
USE_OPENAI_EMBEDDINGS=false  # true for OpenAI embeddings
EMBEDDING_MODEL=all-MiniLM-L6-v2  # Local model
OPENAI_EMBEDDING_MODEL=text-embedding-ada-002
OPENAI_EMBEDDING_API_KEY=...
OPENAI_EMBEDDING_SSO_TOKEN=...
OPENAI_EMBEDDING_BATCH_SIZE=32
```

---
[CODE BLOCK - Languages: bash]


====================================================================================================
CHUNK #25
====================================================================================================
Chunk ID: 1dae6755d4f8a8a042808e90402c0cbbde666d42_merged
Kind: merged_section
Lines: 695-720
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Performance Characteristics

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Performance Characteristics

## Performance Characteristics

Section: AI Chat Application - Complete System Architecture & Flow > Performance Characteristics > Startup Time

### Startup Time
- **Local Embeddings**: 3-5 seconds (model loading)
- **OpenAI Embeddings**: < 1 second (no model loading)
- **LLM Loading**: Varies by provider (Ollama: 2-3s, LlamaCPP: 5-10s)

Section: AI Chat Application - Complete System Architecture & Flow > Performance Characteristics > Response Time

### Response Time
- **Without RAG**: 1-3 seconds for first token
- **With Simple RAG**: +0.5-1 second for retrieval
- **With Fusion Retrieval**: +1-2 seconds for multi-strategy retrieval
  - Vector search: ~200ms
  - BM25 search: ~100ms
  - Multi-query expansion: ~500ms (LLM call)
  - RRF combination: ~50ms

Section: AI Chat Application - Complete System Architecture & Flow > Performance Characteristics > Embedding Performance

### Embedding Performance
- **Local**: ~100-200 chunks/second
- **OpenAI**: ~1000 chunks/second (with batching)

Section: AI Chat Application - Complete System Architecture & Flow > Performance Characteristics > Memory Usage

### Memory Usage
- **Backend**: 2-4 GB (with local models)
- **ChromaDB**: Grows with document count (~1MB per 1000 chunks)

---


====================================================================================================
CHUNK #26
====================================================================================================
Chunk ID: 33622b442ff043652b8f3da19a4c330162dafdbb_merged
Kind: merged_section
Lines: 721-742
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Error Handling

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Error Handling

## Error Handling

Section: AI Chat Application - Complete System Architecture & Flow > Error Handling > Common Scenarios

### Common Scenarios
1. **LLM Provider Unavailable**
   - Backend logs error, returns 503
   - Frontend shows "LLM model not loaded" message

2. **Embedding Failure**
   - Falls back to non-RAG mode
   - Logs warning, continues without context

3. **ChromaDB Connection Issues**
   - Retries connection
   - If persistent: Disables RAG features

4. **File Upload Errors**
   - Returns specific error message (unsupported type, empty file, etc.)
   - Frontend displays error to user

---


====================================================================================================
CHUNK #27
====================================================================================================
Chunk ID: 1f02bc0a44cd762c50f6572e6703ff94ad547a96
Kind: steps
Lines: 743-751
Has Code: False
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Security Considerations

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Security Considerations

## Security Considerations
1. **File Uploads**: Validated extensions, sanitized filenames
2. **API Keys**: Stored in environment variables, never exposed to frontend
3. **SSO Tokens**: Supported for enterprise authentication
4. **CORS**: Configured for specific origins only

---


====================================================================================================
CHUNK #28
====================================================================================================
Chunk ID: b518bc3760ad773a33cc5fedd75cc7022b00214c
Kind: steps
Lines: 752-760
Has Code: False
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Scalability Notes

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Scalability Notes

## Scalability Notes
1. **Horizontal Scaling**: Backend is stateless (except ChromaDB)
2. **ChromaDB**: Can be replaced with cloud vector DB (Pinecone, Weaviate)
3. **Session Storage**: Can be moved to Redis for distributed systems
4. **LLM Providers**: Can load balance across multiple instances

---


====================================================================================================
CHUNK #29
====================================================================================================
Chunk ID: ff992602cab6d9d74575740825c5f67177bd76a2_merged
Kind: merged_section
Lines: 761-777
Has Code: True
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Monitoring & Observability

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Monitoring & Observability

## Monitoring & Observability

Section: AI Chat Application - Complete System Architecture & Flow > Monitoring & Observability > Logs to Monitor

### Logs to Monitor
1. **Startup**: Provider initialization, model loading
2. **Runtime**: Chat requests, RAG retrievals, errors
3. **Performance**: Token counts, processing times, embedding speeds

Section: AI Chat Application - Complete System Architecture & Flow > Monitoring & Observability > Key Metrics

### Key Metrics
1. **Response Time**: Track p50, p95, p99
2. **Token Usage**: Monitor for cost control
3. **RAG Hit Rate**: How often RAG finds relevant docs
4. **Error Rate**: Track provider failures

---


====================================================================================================
CHUNK #30
====================================================================================================
Chunk ID: b3fae49b9f02bd053c047a2f7acd972a413b5a1c
Kind: section
Lines: 778-804
Has Code: False
Header Level: 2
Section Path: AI Chat Application - Complete System Architecture & Flow > Conclusion

----------------------------------------------------------------------------------------------------
CONTENT:
----------------------------------------------------------------------------------------------------

Section: AI Chat Application - Complete System Architecture & Flow > Conclusion

## Conclusion

This application provides a flexible, extensible AI chat system with advanced RAG features, particularly **Fusion Retrieval using Reciprocal Rank Fusion (RRF)**. The modular architecture allows easy swapping of LLM providers and embedding models, making it suitable for both development and production environments.

**Key Strengths:**
- **Advanced Retrieval**: Fusion of vector, BM25, and multi-query strategies
- **Real-time Streaming**: Responsive UX with SSE
- **Flexible Provider System**: Support for multiple LLM and embedding providers
- **Comprehensive Memory**: Session-based conversation history
- **Source Diversity**: Ensures broad coverage across documents
- **Detailed Statistics**: Full transparency on retrieval and generation
- **Production-Ready**: Robust error handling and performance optimization

**Fusion Retrieval Benefits:**
- Combines semantic understanding (vector) with keyword matching (BM25)
- Query expansion captures different phrasings and perspectives
- RRF algorithm provides balanced, hybrid ranking
- Source diversity prevents over-reliance on single documents
- Better handles multi-document, complex queries

**Future Enhancements:**
- Agentic RAG with tool-based decision making
- Re-ranking models for result refinement
- Hybrid search with filtering and metadata
- Authentication and user management
- Advanced analytics dashboard
- Multi-modal support (images, audio)

