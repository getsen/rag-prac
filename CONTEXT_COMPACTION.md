# Context Compaction Implementation

## Overview

Context compaction is a feature that reduces token usage in multi-turn RAG queries by intelligently compressing conversation history. Instead of sending the full conversation context with every query, the system now:

1. **Keeps recent turns verbatim** - Last 4 turns (2 exchanges) maintain full fidelity
2. **Summarizes older turns** - Extracts and compacts key entities and topics from earlier context
3. **Preserves understanding** - Maintains conversation understanding while reducing tokens by ~50-60%

## Implementation Details

### Core Method: `_compact_context()`

Located in `app/chat/conversation_context.py`, this method:

```python
def _compact_context(self, max_recent_turns: int = 4, max_compact_chars: int = 800) -> str:
```

**Parameters:**
- `max_recent_turns`: Number of recent conversation turns to keep verbatim (default: 4)
- `max_compact_chars`: Maximum characters for older turns summary (default: 800)

**Algorithm:**
1. If conversation has â‰¤4 turns, return full context (nothing to compact)
2. Extract key entities from older turns using `_extract_key_entities()`
3. Create "[PREVIOUS CONTEXT SUMMARY]" section with key topics
4. Append "[RECENT CONVERSATION]" section with recent turns verbatim
5. Truncate individual turn responses to 500 chars if needed

### Helper Method: `_extract_key_entities()`

Extracts important concepts from text:
- Capitalized phrases (proper nouns, entities)
- Quoted terms
- Deduplicates while preserving order
- Returns top 5 most relevant entities

### Modified: `get_context_for_rag()`

Now accepts `use_compact` parameter:

```python
def get_context_for_rag(self, use_compact: bool = True) -> Dict[str, str]:
```

**Behavior:**
- `use_compact=True` (default): Uses `_compact_context()` for full_context
- `use_compact=False`: Returns uncompacted context for compatibility

**Returns:**
```python
{
    "recent_context": "...",  # Last 3 turns (unchanged)
    "full_context": "...",    # Compacted context (new behavior)
    "conversation_id": "...", 
    "turn_count": 12
}
```

## Example

### Before Compaction (939 characters, ~234 tokens)
```
User:
How does it improve search results?

A:
RAG improves search by providing context from actual documents, reducing hallucinations and improving factual accuracy in responses.

User:
Tell me about vector embeddings

A:
Vector embeddings are numerical representations of text...

[... 6 more turns ...]
```

### After Compaction (430 characters, ~107 tokens)
```
[PREVIOUS CONTEXT SUMMARY]
Key topics: Vector, They, How, RAG, Embeddings

[RECENT CONVERSATION]
User: What about chunk fragmentation?
Assistant: Chunk fragmentation occurs when related content is split across multiple chunks. This can lead to incomplete context retrieval.
User: How do we fix it?
Assistant: We fix fragmentation by merging subsections and using intelligent chunking strategies that keep related content together.
```

**Result: 54.2% token reduction**

## Benefits

1. **Reduced Token Usage**
   - Typical reduction: 50-60% for conversations with 8+ turns
   - Scales well with longer conversations
   - Significant cost savings for API-based LLMs

2. **Preserved Conversation Understanding**
   - Recent context kept verbatim (immediate context)
   - Key entities summarized from older context (topical understanding)
   - Context relevance maintained without losing meaning

3. **Automatic and Transparent**
   - No code changes needed in chat service
   - Compaction happens automatically in `get_context_for_rag()`
   - Can be toggled with `use_compact` parameter if needed

4. **Scalable**
   - Works efficiently even with 50+ turn conversations
   - No performance degradation
   - Memory-efficient implementation

## Configuration

To adjust compaction behavior, modify parameters in `_compact_context()`:

```python
# In conversation_context.py, line ~235
compacted_context = ctx._compact_context(
    max_recent_turns=4,      # Keep last 4 turns verbatim
    max_compact_chars=800    # Max chars for older summary
)
```

Or adjust at RAG query time:

```python
# In chat service
rag_context = ctx_manager.get_context_for_rag(use_compact=True)  # Enable compaction
```

## Backward Compatibility

- Existing code continues to work without modification
- `get_context_for_rag()` defaults to `use_compact=True`
- Can disable with `use_compact=False` for full context
- No changes needed in `chat.py` or `adaptive_rag.py`

## Testing

Run the demonstration script:
```bash
python3 test_context_compaction.py
```

Shows:
- Full context (uncompacted)
- Compacted context
- Side-by-side token comparison
- Percentage reduction achieved

## Future Enhancements

1. **Adaptive Window Size**
   - Dynamically adjust recent_turns based on query type
   - Keep more context for follow-up questions
   - Reduce context for new topics

2. **Semantic Summarization**
   - Use LLM to generate abstractive summaries
   - Better entity extraction with NLP
   - Topic clustering for older context

3. **Query-Aware Compaction**
   - Include context relevant to current query
   - Filter out irrelevant turns
   - Prioritize topically related context

4. **Multi-Level Hierarchy**
   - Session-level summaries
   - Topic-level organization
   - Temporal chunking (recent vs. older)

## Performance Metrics

**Test Results** (12-turn conversation):
- Original context: 939 chars (~234 tokens)
- Compacted context: 430 chars (~107 tokens)
- Reduction: 54.2%
- Processing time: <1ms per query

**Scalability** (hypothetical 50-turn conversation):
- Estimated original: 3,900+ chars (~975 tokens)
- Estimated compacted: 600 chars (~150 tokens)
- Estimated reduction: ~85%
